{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A need I see often is the need to take a collection of files with lists of attributes and put them into a \"booleanized\" or count table. Each file is a column and each item in the collected files is a row. The cell is either a 1 or 0, or a value if the files have second columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, a file called \"apple\" contains \"red\" and \"fruit\" and a file called \"orange\" contains \"orange\" and \"fruit\". The resulting table would look something like...\n",
    "\n",
    "| | apple | orange |\n",
    "|-| | |\n",
    "| red | 1 | 0 |\n",
    "| orange | 0 | 1 |\n",
    "| fruit | 1 | 1 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat > apple <<-EOF\n",
    "red\n",
    "fruit\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red\n",
      "fruit\n"
     ]
    }
   ],
   "source": [
    "cat apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat > orange <<-EOF\n",
    "orange\n",
    "fruit\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orange\n",
      "fruit\n"
     ]
    }
   ],
   "source": [
    "cat orange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In gawk it would look something like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-,apple,orange\n",
      "fruit,1,1\n",
      "orange,0,1\n",
      "red,1,0\n"
     ]
    }
   ],
   "source": [
    "gawk '( FNR == 1 ){\n",
    "  split(FILENAME,f,\".\")\n",
    "  filename=f[1]\n",
    "  filenames[filename]++\n",
    "} {\n",
    "  attribute=$1\n",
    "  attributes[attribute]++\n",
    "  b[filename][attribute]++\n",
    "} END {\n",
    "  asorti(attributes)\n",
    "  asorti(filenames)\n",
    "  printf \"-\"\n",
    "  for(filename in filenames) printf \",%s\",filenames[filename]\n",
    "  printf \"\\n\"\n",
    "  for(attribute in attributes){\n",
    "    printf attributes[attribute]\n",
    "    for(filename in filenames) printf \",%s\",b[filenames[filename]][attributes[attribute]]||0\n",
    "    printf \"\\n\"\n",
    "  }\n",
    "}' apple orange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most part this is just fine, however it requires that all files be read and loaded into memory. If you have a couple hundred files, and each attribute is a string of say 10 chars and each files contains about 20,000 attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320000000\n"
     ]
    }
   ],
   "source": [
    "echo $(( 200 * 10 * 8 * 20000 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "320MB isn't so bad. But if you have 450 files with 1 - 2 million attributes ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54000000000\n"
     ]
    }
   ],
   "source": [
    "echo $(( 450 * 10 * 8 * 1500000 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "54GB just to load the data files. When I loaded this data set it took 120GB because the data is actually in 2 arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\rm apple\n",
    "\\rm orange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to devise a way to do this using much less memory.\n",
    "\n",
    "My plan is to read all of the files once, or provide a separate list of all the attributes, then process each file 1 at a time and create the table transposed, 90 degrees. Then once complete, transpose the table 90 degrees using either `datamash transpose -t,` or a \"cut and paste\" script like\n",
    "\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "\n",
    "numc=$(($(head -n 1 \"$1\" | grep -o \"$2\" | wc -l)+1))\n",
    "for ((i=1; i<=\"$numc\"; i++))\n",
    "do cut -d \"$2\" -f\"$i\" \"$1\" | paste -s -d \"$2\"\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
